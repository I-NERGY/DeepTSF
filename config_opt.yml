hyperparameters:

  transformer_test:
    input_chunk_length: ["list", 6, 12, 24, 48, 76]
    output_chunk_length: 1
    d_model: ["list", 32, 64]
    nhead: ["list", 2, 3]
    num_encoder_layers: 3
    num_decoder_layers: ["range", 1, 3, 1]
    dim_feedforward: 512
    dropout: 0.1 
    activation: 'relu'
    n_epochs: 2
    random_state: 0
    nr_epochs_val_period: 2
    batch_size: 32

  nhits_test_new:
    input_chunk_length: ["list", 6, 12, 24, 48, 76]
    output_chunk_length: 1
    num_stacks: ["range", 1, 5, 1]
    num_blocks: ["range", 1, 5, 1]
    num_layers: ["range", 1, 3, 1]
    layer_widths: 64
    n_epochs: 2
    random_state: 0
    nr_epochs_val_period: 2
    batch_size: 32

  lstm_test:
     model: LSTM
     n_rnn_layers: ["list", 1, 2, 3]
     hidden_dim:  ["list", 32, 64]
     n_epochs: 50
     input_chunk_length: 6
     training_length: 6
     random_state: 0
     nr_epochs_val_period: 2
     dropout: 0.3
     learning_rate: ["list", 0.01, 0.001]
     # likelihood: Gaussian
     batch_size: ["list", 16, 32]


  TCN_val_2020:
    output_chunk_length: 24
    weight_norm: True
    dropout: 0
    nr_epochs_val_period: 2
    n_epochs: 500
    random_state: 0
    input_chunk_length: ["range", 48, 480, 24]
    kernel_size: ["range", 2, 6, 1]
    num_filters: ["range", 2, 12, 1]
    dilation_base: ["list", 2, 4, 8, 16, 32]
    batch_size: ["list", 256, 512, 1024, 1280, 1536, 2048]

#An ta layers bgainoun symantikotero karfwse ta 4
#kai 19 kai 20
  NBEATS_thesis:
    input_chunk_length: ["range", 120, 312, 24]
    output_chunk_length: 24
    num_stacks: ["range", 1, 10, 1]
    num_blocks: ["range", 1, 6, 1]
    num_layers: ["range", 1, 10, 1]
    generic_architecture: True
    layer_widths: 64
    expansion_coefficient_dim: 5
    n_epochs: 200
    random_state: 0
    nr_epochs_val_period: 2
    batch_size: 1024

  NBEATS_thesis_multiple:
    input_chunk_length: ["range", 120, 312, 24]
    output_chunk_length: 24
    num_stacks: ["range", 1, 10, 1]
    num_blocks: ["range", 1, 6, 1]
    num_layers: ["range", 1, 10, 1]
    generic_architecture: True
    layer_widths: 64
    expansion_coefficient_dim: 5
    n_epochs: 200
    random_state: 0
    nr_epochs_val_period: 2
    batch_size: 1024
    scale:  ["list", "True", "False"]

  NBEATS_uc7_mase:
    input_chunk_length: ["range", 120, 312, 24]
    output_chunk_length: 24
    num_stacks: ["range", 1, 10, 1]
    num_blocks: ["range", 1, 6, 1]
    num_layers: ["range", 1, 10, 1]
    generic_architecture: True
    layer_widths: 64
    expansion_coefficient_dim: 5
    n_epochs: 200
    random_state: 0
    nr_epochs_val_period: 2
    batch_size: 1024

  LightGBM_thesis_multiple:
    lags: ["range", 120, 312, 24]
    lags_past_covariates: null
    lags_future_covariates: None
    future_covs_as_tuple: true
    random_state: 0
    scale: ["list", "True", "False"]

  NBEATS_testt:
    input_chunk_length: ["range", 120, 312, 24]
    output_chunk_length: 24
    num_stacks: ["range", 1, 10, 1]
    num_blocks: ["range", 1, 6, 1]
    num_layers: ["range", 1, 10, 1]
    generic_architecture: True
    layer_widths: 64
    expansion_coefficient_dim: 5
    n_epochs: 2
    random_state: 0
    nr_epochs_val_period: 2
    batch_size: 1024

  BlockLSTM_val_2020_:
    model: LSTM
    n_rnn_layers: ["range", 1, 4, 1] #kospe an yparxei thema
    input_chunk_length: ["range", 24, 480, 24]
    output_chunk_length: 24
    hidden_dim:  ["range", 24, 240, 24] #hmerisio???
    n_epochs: 2
    random_state: 0
    nr_epochs_val_period: 2
    dropout: 0
    learning_rate: 0.001
    batch_size: ["list", 256, 512, 1024, 1280, 1536, 2048]
    scale:  ["list", "True", "False"]

  MLP_val_2020:
    hidden_layer_sizes : # n_layers=[1,10, 1], n_neur=[50, 200, 10]
    activation: ["list", "relu", "logistic"]
    solver: 'adam'
    alpha: 0.0001
    batch_size: 'auto' #print and save the batch size
    learning_rate: 'constant'
    learning_rate_init: ["range", 0.0005, 0.002, 0.0005] #asto karfwmeno
    max_iter: 200
    shuffle: ["list", "True", "False"]
    random_state: 0
    verbose: True
    early_stopping: True
    validation_fraction: 0.13
    tolerance: 1e-06
    n_iter_no_change: 10
                          #kai lbw bale
                          #trekse apo script
  NBEATS_val_2020_5_13:
    input_chunk_length: ["range", 120, 312, 24]
    output_chunk_length: 24
    num_stacks: ["range", 1, 25, 1]
    num_blocks: ["range", 1, 20, 1]
    num_layers: ["range", 1, 10, 1]
    generic_architecture: True
    layer_widths: 64
    expansion_coefficient_dim: 5
    n_epochs: 300
    random_state: 0
    nr_epochs_val_period: 2
    batch_size: ["list", 256, 512, 1024, 1280, 1536, 2048]

  NBEATS_val_2020_17_23__:
    input_chunk_length: ["range", 408, 552, 24]
    output_chunk_length: 24
    num_stacks: ["range", 1, 25, 1]
    num_blocks: ["range", 1, 20, 1]
    num_layers: ["range", 1, 10, 1]
    generic_architecture: True
    layer_widths: 64
    expansion_coefficient_dim: 5
    n_epochs: 20
    random_state: 0
    nr_epochs_val_period: 2
    batch_size: ["list", 256, 512, 1024, 1280, 1536, 2048]

  NBEATS_15min:
    input_chunk_length: ["range", 96, 672, 96]
    output_chunk_length: 96
    num_stacks: ["range", 1, 20, 2]
    num_blocks: ["range", 1, 5, 1]
    num_layers: ["range", 1, 5, 1]
    generic_architecture: True
    layer_widths: 64
    expansion_coefficient_dim: 5
    n_epochs: 500
    random_state: 0
    nr_epochs_val_period: 2
    batch_size: 1024

  NHiTS_15min:
    input_chunk_length: ["range", 96, 672, 96]
    output_chunk_length: 96
    num_stacks: ["range", 1, 20, 2]
    num_blocks: ["range", 1, 5, 1]
    num_layers: ["range", 1, 5, 1]
    layer_widths: 64
    n_epochs: 500
    random_state: 0
    nr_epochs_val_period: 2
    batch_size: 1024

  TCN_15min:
    output_chunk_length: 96
    weight_norm: True
    dropout: 0
    nr_epochs_val_period: 2
    n_epochs: 500
    random_state: 0
    input_chunk_length: ["range", 192, 672, 96]
    kernel_size: ["range", 2, 5, 1]
    num_filters: ["range", 2, 6, 1]
    dilation_base: ["list", 2, 3, 4]
    batch_size: 1024

  BlockLSTM_15min:
    model: LSTM
    n_rnn_layers: ["range", 2, 4, 1]
    input_chunk_length: ["range", 96, 672, 96]
    output_chunk_length: 96
    hidden_dim:  ["list", 16, 32, 64, 128] 
    n_epochs: 500
    random_state: 0
    nr_epochs_val_period: 2
    dropout: 0
    learning_rate: 0.001
    batch_size: 1024
    
  MLP_15min:
    #hidden_layer_sizes : # n_layers=[1, 4, 1], n_neur=[16, 32, 64, 128, 256]
    #activation: relu
    #solver: 'adam'
    #alpha: 0.0001
    #batch_size: 'auto' #print and save the batch size
    #learning_rate: 'constant'
    #learning_rate_init: ["range", 0.0005, 0.002, 0.0005] #asto karfwmeno
    #max_iter: 200
    #shuffle: ["list", "True", "False"]
    #random_state: 0
    #verbose: True
    #early_stopping: True
    #validation_fraction: 0.13
    #tolerance: 1e-06
    n_iter_no_change: 10

  nbeats_test_3:
    input_chunk_length: ["range", 3, 12, 3]
    output_chunk_length: 1
    num_stacks: ["range", 1, 12, 1]
    num_blocks: ["range", 1, 12, 1]
    num_layers: ["range", 1, 12, 1]
    generic_architecture: True
    layer_widths: 64
    expansion_coefficient_dim: 5
    n_epochs: 3
    random_state: 0
    nr_epochs_val_period: 2
    batch_size: ["list", 8, 16, 32, 64]

  lgbm_3d:
    lags: ["list", 48, 24]
    lags_past_covariates: null
    lags_future_covariates: [1, 1]
    future_covs_as_tuple: true
    random_state: 0
