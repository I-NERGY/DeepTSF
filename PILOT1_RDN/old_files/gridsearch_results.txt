hparams = \
    {
        'model': ['LSTM'],
        'torch_device_str': ['cuda'],
        'log_tensorboard': [True],
        'save_checkpoints': [True],
        'optimizer_kwargs': [{'lr': 0.003}, {'lr': 0.0001}, {'lr': 0.00001}],
        'n_rnn_layers': [2, 3, 4, 6],
        'hidden_dim': [8, 64, 128, 256],
        'n_epochs': [100],
        'input_chunk_length':[8, 64, 128, 512],
        'training_length': [8, 64, 128, 512],  # should be >= input_chunk
        'random_state': [42],
        'nr_epochs_val_period': [1],
        'dropout': [0],
        'batch_size': [32]
    }

state_dict = {'model': 'LSTM',
 'torch_device_str': 'cuda',
 'log_tensorboard': True,
 'save_checkpoints': True,
 'optimizer_kwargs': {'lr': 1e-05},
 'n_rnn_layers': 3,
 'hidden_dim': 8,
 'n_epochs': 100,
 'input_chunk_length': 8,
 'training_length': 128,
 'random_state': 42,
 'nr_epochs_val_period': 1,
 'dropout': 0,
 'batch_size': 32}